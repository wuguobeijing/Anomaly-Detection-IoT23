Presets specified: ['optimize_for_deployment']
============ fit kwarg info ============
User Specified kwargs:
{'ag_args_fit': {'num_gpus': 1},
 'hyperparameter_tune_kwargs': {'num_trials': 50,
                                'scheduler': 'local',
                                'searcher': 'auto'},
 'keep_only_best': True,
 'num_bag_folds': 5,
 'num_bag_sets': 1,
 'num_stack_levels': 1,
 'save_space': True,
 'verbosity': 4}
Full kwargs:
{'_feature_generator_kwargs': None,
 '_save_bag_folds': None,
 'ag_args': None,
 'ag_args_ensemble': None,
 'ag_args_fit': {'num_gpus': 1},
 'auto_stack': False,
 'calibrate': 'auto',
 'excluded_model_types': None,
 'feature_generator': 'auto',
 'feature_prune_kwargs': None,
 'holdout_frac': None,
 'hyperparameter_tune_kwargs': {'num_trials': 50,
                                'scheduler': 'local',
                                'searcher': 'auto'},
 'keep_only_best': True,
 'name_suffix': None,
 'num_bag_folds': 5,
 'num_bag_sets': 1,
 'num_stack_levels': 1,
 'pseudo_data': None,
 'quantile_levels': None,
 'refit_full': False,
 'save_space': True,
 'set_best_to_refit_full': False,
 'unlabeled_data': None,
 'use_bag_holdout': False,
 'verbosity': 4}
========================================
Warning: hyperparameter tuning is currently experimental and may cause the process to hang.
Saving ./auto_gl/exp-iot23-oversample/deploy-exp/STTACKING2_layer2_search/learner.pkl
Saving ./auto_gl/exp-iot23-oversample/deploy-exp/STTACKING2_layer2_search/predictor.pkl
Beginning AutoGluon training ... Time limit = 36000s
AutoGluon will save models to "./auto_gl/exp-iot23-oversample/deploy-exp/STTACKING2_layer2_search/"
AutoGluon Version:  0.4.0
Python Version:     3.8.13
Operating System:   Linux
Train Data Rows:    5469674
Train Data Columns: 30
Label Column: label
Preprocessing data ...
AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).
	2 unique label values:  ['Benign', 'Malicious']
	If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])
Selected class <--> label mapping:  class 1 = Malicious, class 0 = Benign
	Note: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (Malicious) vs negative (Benign) class.
	To explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.
Using Feature Generators to preprocess the data ...
Fitting AutoMLPipelineFeatureGenerator...
	Available Memory:                    7720.88 MB
	Train Data (Original)  Memory Usage: 1312.72 MB (17.0% of available memory)
	Warning: Data size prior to feature transformation consumes 17.0% of available memory. Consider increasing memory or subsampling the data to avoid instability.
	Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.
	Stage 1 Generators:
		Fitting AsTypeFeatureGenerator...
			Note: Converting 16 features to boolean dtype as they only contain 2 unique values.
			Original Features (exact raw dtype, raw dtype):
				('float64', 'float') : 30 | ['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes', 'orig_pkts', ...]
			Types of features in original data (raw dtype, special dtypes):
				('float', []) : 30 | ['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes', 'orig_pkts', ...]
			Types of features in processed data (exact raw dtype, raw dtype):
				('float64', 'float') : 14 | ['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes', 'orig_pkts', ...]
				('int8', 'int')      : 16 | ['proto_tcp', 'service_dhcp', 'service_http', 'service_ssh', 'service_ssl', ...]
			Types of features in processed data (raw dtype, special dtypes):
				('float', [])     : 14 | ['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes', 'orig_pkts', ...]
				('int', ['bool']) : 16 | ['proto_tcp', 'service_dhcp', 'service_http', 'service_ssh', 'service_ssl', ...]
			15.7s = Fit runtime
			30 features in original data used to generate 30 features in processed data.
	Stage 2 Generators:
		Fitting FillNaFeatureGenerator...
			Types of features in original data (raw dtype, special dtypes):
				('float', [])     : 14 | ['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes', 'orig_pkts', ...]
				('int', ['bool']) : 16 | ['proto_tcp', 'service_dhcp', 'service_http', 'service_ssh', 'service_ssl', ...]
			Types of features in processed data (exact raw dtype, raw dtype):
				('float64', 'float') : 14 | ['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes', 'orig_pkts', ...]
				('int8', 'int')      : 16 | ['proto_tcp', 'service_dhcp', 'service_http', 'service_ssh', 'service_ssl', ...]
			Types of features in processed data (raw dtype, special dtypes):
				('float', [])     : 14 | ['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes', 'orig_pkts', ...]
				('int', ['bool']) : 16 | ['proto_tcp', 'service_dhcp', 'service_http', 'service_ssh', 'service_ssl', ...]
			1.0s = Fit runtime
			30 features in original data used to generate 30 features in processed data.
	Stage 3 Generators:
		Fitting IdentityFeatureGenerator...
			Types of features in original data (raw dtype, special dtypes):
				('float', [])     : 14 | ['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes', 'orig_pkts', ...]
				('int', ['bool']) : 16 | ['proto_tcp', 'service_dhcp', 'service_http', 'service_ssh', 'service_ssl', ...]
			Types of features in processed data (exact raw dtype, raw dtype):
				('float64', 'float') : 14 | ['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes', 'orig_pkts', ...]
				('int8', 'int')      : 16 | ['proto_tcp', 'service_dhcp', 'service_http', 'service_ssh', 'service_ssl', ...]
			Types of features in processed data (raw dtype, special dtypes):
				('float', [])     : 14 | ['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes', 'orig_pkts', ...]
				('int', ['bool']) : 16 | ['proto_tcp', 'service_dhcp', 'service_http', 'service_ssh', 'service_ssl', ...]
			0.9s = Fit runtime
			30 features in original data used to generate 30 features in processed data.
		Skipping CategoryFeatureGenerator: No input feature with required dtypes.
		Skipping DatetimeFeatureGenerator: No input feature with required dtypes.
		Skipping TextSpecialFeatureGenerator: No input feature with required dtypes.
		Skipping TextNgramFeatureGenerator: No input feature with required dtypes.
		Skipping IdentityFeatureGenerator: No input feature with required dtypes.
		Skipping IsNanFeatureGenerator: No input feature with required dtypes.
	Stage 4 Generators:
		Fitting DropUniqueFeatureGenerator...
			Types of features in original data (raw dtype, special dtypes):
				('float', [])     : 14 | ['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes', 'orig_pkts', ...]
				('int', ['bool']) : 16 | ['proto_tcp', 'service_dhcp', 'service_http', 'service_ssh', 'service_ssl', ...]
			Types of features in processed data (exact raw dtype, raw dtype):
				('float64', 'float') : 14 | ['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes', 'orig_pkts', ...]
				('int8', 'int')      : 16 | ['proto_tcp', 'service_dhcp', 'service_http', 'service_ssh', 'service_ssl', ...]
			Types of features in processed data (raw dtype, special dtypes):
				('float', [])     : 14 | ['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes', 'orig_pkts', ...]
				('int', ['bool']) : 16 | ['proto_tcp', 'service_dhcp', 'service_http', 'service_ssh', 'service_ssl', ...]
			2.1s = Fit runtime
			30 features in original data used to generate 30 features in processed data.
	Types of features in original data (exact raw dtype, raw dtype):
		('float64', 'float') : 30 | ['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes', 'orig_pkts', ...]
	Types of features in original data (raw dtype, special dtypes):
		('float', []) : 30 | ['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes', 'orig_pkts', ...]
	Types of features in processed data (exact raw dtype, raw dtype):
		('float64', 'float') : 14 | ['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes', 'orig_pkts', ...]
		('int8', 'int')      : 16 | ['proto_tcp', 'service_dhcp', 'service_http', 'service_ssh', 'service_ssl', ...]
	Types of features in processed data (raw dtype, special dtypes):
		('float', [])     : 14 | ['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes', 'orig_pkts', ...]
		('int', ['bool']) : 16 | ['proto_tcp', 'service_dhcp', 'service_http', 'service_ssh', 'service_ssl', ...]
	23.2s = Fit runtime
	30 features in original data used to generate 30 features in processed data.
	Train Data (Processed) Memory Usage: 700.12 MB (9.1% of available memory)
Data preprocessing and feature engineering runtime = 26.56s ...
AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'
	This metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()
	To change this, specify the eval_metric parameter of Predictor()
Saving ./auto_gl/exp-iot23-oversample/deploy-exp/STTACKING2_layer2_search/learner.pkl
Saving ./auto_gl/exp-iot23-oversample/deploy-exp/STTACKING2_layer2_search/utils/data/X.pkl
Saving ./auto_gl/exp-iot23-oversample/deploy-exp/STTACKING2_layer2_search/utils/data/y.pkl
AutoGluon will fit 2 stack levels (L1 to L2) ...
Model configs that will be trained (in order):
	LightGBM_BAG_L1: 	{'learning_rate': 0.103335706015396, 'num_boost_round': 100, 'num_leaves': 62, 'feature_fraction': 0.842181292665241, 'min_data_in_leaf': 2, 'ag_args': {'hyperparameter_tune_kwargs': {'num_trials': 50, 'scheduler': 'local', 'searcher': 'auto'}, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}, 'ag_args_fit': {'num_gpus': 1}}
	CatBoost_BAG_L1: 	{'iterations': 10000, 'learning_rate': 0.02057151708150835, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'depth': 5, 'l2_leaf_reg': 4.854651042004117, 'ag_args': {'hyperparameter_tune_kwargs': {'num_trials': 50, 'scheduler': 'local', 'searcher': 'auto'}, 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}, 'ag_args_fit': {'num_gpus': 1}}
Fitting 2 L1 models ...
Hyperparameter tuning model: LightGBM_BAG_L1 ... Tuning model for up to 2157.87s of the 35973.44s of remaining time.
	Dropped 0 of 30 features.
	Dropped 0 of 30 features.
	Dropped 0 of 30 features.
	Warning: Potentially not enough memory to safely train model, roughly requires: 4.463 GB, but only 7.36 GB is available...
Starting generic AbstractModel hyperparameter tuning for LightGBM model...
	No hyperparameter search space specified for LightGBM. Skipping HPO. Will train one model based on the provided hyperparameters.
	Fitting LightGBM with 'num_gpus': 1, 'num_cpus': 8
	Warning: Potentially not enough memory to safely train model, roughly requires: 4.463 GB, but only 7.36 GB is available...
	Training LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.
	Fitting 100 rounds... Hyperparameters: {'learning_rate': 0.103335706015396, 'num_leaves': 62, 'feature_fraction': 0.842181292665241, 'min_data_in_leaf': 2, 'device': 'gpu'}
[LightGBM] [Fatal] Check failed: (best_split_info.left_count) > (0) at /home/wuguo-buaa/tmp/pip-install-4flbbpn1/lightgbm_eb6c31e6ac6b49c59f7cb6cbd6b868e8/compile/src/treelearner/serial_tree_learner.cpp, line 653 .

Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:	pip uninstall lightgbm -y	pip install lightgbm --install-option=--gpu
Saving ./auto_gl/exp-iot23-oversample/deploy-exp/STTACKING2_layer2_search/models/LightGBM_BAG_L1/hpo/model.pkl
Loading: ./auto_gl/exp-iot23-oversample/deploy-exp/STTACKING2_layer2_search/models/LightGBM_BAG_L1/hpo/model.pkl
Saving ./auto_gl/exp-iot23-oversample/deploy-exp/STTACKING2_layer2_search/models/LightGBM_BAG_L1/T1/utils/model_template.pkl
Saving ./auto_gl/exp-iot23-oversample/deploy-exp/STTACKING2_layer2_search/models/LightGBM_BAG_L1/T1/utils/oof.pkl
Saving ./auto_gl/exp-iot23-oversample/deploy-exp/STTACKING2_layer2_search/models/LightGBM_BAG_L1/T1/model.pkl
Loading: ./auto_gl/exp-iot23-oversample/deploy-exp/STTACKING2_layer2_search/models/LightGBM_BAG_L1/T1/model.pkl
Fitted model: LightGBM_BAG_L1/T1 ...
	0.9953	 = Validation score   (roc_auc)
	20.66s	 = Training   runtime
	0.86s	 = Validation runtime
Saving ./auto_gl/exp-iot23-oversample/deploy-exp/STTACKING2_layer2_search/models/trainer.pkl
Hyperparameter tuning model: CatBoost_BAG_L1 ... Tuning model for up to 2157.87s of the 35947.05s of remaining time.
	Dropped 0 of 30 features.
	Dropped 0 of 30 features.
	Dropped 0 of 30 features.
	Warning: Potentially not enough memory to safely train CatBoost model, roughly requires: 4.463 GB, but only 7.183 GB is available...
Starting generic AbstractModel hyperparameter tuning for CatBoost model...
	No hyperparameter search space specified for CatBoost. Skipping HPO. Will train one model based on the provided hyperparameters.
	Fitting CatBoost with 'num_gpus': 1, 'num_cpus': 8
	Warning: Potentially not enough memory to safely train CatBoost model, roughly requires: 4.463 GB, but only 7.183 GB is available...
	Training CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.
	Catboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.02057151708150835, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'depth': 5, 'l2_leaf_reg': 4.854651042004117, 'thread_count': 8, 'task_type': 'GPU'}
	Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
You should provide test set for use best model. use_best_model parameter has been switched to false value.

	Dropped 0 of 32 features.
	Fitting S1F3 with 'num_gpus': 1, 'num_cpus': 8
	Warning: Potentially not enough memory to safely train model, roughly requires: 4.726 GB, but only 5.504 GB is available...
	Training S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.
	Fitting 100 rounds... Hyperparameters: {'learning_rate': 0.103335706015396, 'num_leaves': 62, 'feature_fraction': 0.842181292665241, 'min_data_in_leaf': 2, 'device': 'gpu'}
[LightGBM] [Fatal] Check failed: (best_split_info.left_count) > (0) at /home/wuguo-buaa/tmp/pip-install-4flbbpn1/lightgbm_eb6c31e6ac6b49c59f7cb6cbd6b868e8/compile/src/treelearner/serial_tree_learner.cpp, line 653 .

Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:	pip uninstall lightgbm -y	pip install lightgbm --install-option=--gpu
*** End of fit() summary ***
{'model_types': {'LightGBM_BAG_L1/T1': 'StackerEnsembleModel_LGB', 'CatBoost_BAG_L1/T1': 'StackerEnsembleModel_CatBoost', 'LightGBM_BAG_L2/T1': 'StackerEnsembleModel_LGB', 'CatBoost_BAG_L2/T1': 'StackerEnsembleModel_CatBoost', 'WeightedEnsemble_L3': 'WeightedEnsembleModel'}, 'model_performance': {'LightGBM_BAG_L1/T1': 0.9953438579738467, 'CatBoost_BAG_L1/T1': 0.9953201244735981, 'LightGBM_BAG_L2/T1': 0.9953979477033679, 'CatBoost_BAG_L2/T1': 0.9953974373898427, 'WeightedEnsemble_L3': 0.9954007138816282}, 'model_best': 'WeightedEnsemble_L3', 'model_paths': {'LightGBM_BAG_L1/T1': './auto_gl/exp-iot23-oversample/deploy-exp/STTACKING2_layer2_search/models/LightGBM_BAG_L1/T1/', 'CatBoost_BAG_L1/T1': './auto_gl/exp-iot23-oversample/deploy-exp/STTACKING2_layer2_search/models/CatBoost_BAG_L1/T1/', 'LightGBM_BAG_L2/T1': './auto_gl/exp-iot23-oversample/deploy-exp/STTACKING2_layer2_search/models/LightGBM_BAG_L2/T1/', 'CatBoost_BAG_L2/T1': './auto_gl/exp-iot23-oversample/deploy-exp/STTACKING2_layer2_search/models/CatBoost_BAG_L2/T1/', 'WeightedEnsemble_L3': './auto_gl/exp-iot23-oversample/deploy-exp/STTACKING2_layer2_search/models/WeightedEnsemble_L3/'}, 'model_fit_times': {'LightGBM_BAG_L1/T1': 69.48812365531921, 'CatBoost_BAG_L1/T1': 136.35577583312988, 'LightGBM_BAG_L2/T1': 52.720187187194824, 'CatBoost_BAG_L2/T1': 455.17422699928284, 'WeightedEnsemble_L3': 237.50195240974426}, 'model_pred_times': {'LightGBM_BAG_L1/T1': 2.9392435550689697, 'CatBoost_BAG_L1/T1': 1.0600388050079346, 'LightGBM_BAG_L2/T1': 3.024332284927368, 'CatBoost_BAG_L2/T1': 1.5112643241882324, 'WeightedEnsemble_L3': 1.0045900344848633}, 'num_bag_folds': 5, 'max_stack_level': 3, 'num_classes': 2, 'model_hyperparams': {'LightGBM_BAG_L1/T1': {'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}, 'CatBoost_BAG_L1/T1': {'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}, 'LightGBM_BAG_L2/T1': {'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}, 'CatBoost_BAG_L2/T1': {'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}, 'WeightedEnsemble_L3': {'use_orig_features': False, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}}, 'leaderboard':                  model  score_val  pred_time_val    fit_time  \
0  WeightedEnsemble_L3   0.995401       9.539469  951.240266
1   LightGBM_BAG_L2/T1   0.995398       7.023615  258.564087
2   CatBoost_BAG_L2/T1   0.995397       5.510547  661.018126
3   LightGBM_BAG_L1/T1   0.995344       2.939244   69.488124
4   CatBoost_BAG_L1/T1   0.995320       1.060039  136.355776

   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \
0                1.004590         237.501952            3       True
1                3.024332          52.720187            2       True
2                1.511264         455.174227            2       True
3                2.939244          69.488124            1       True
4                1.060039         136.355776            1       True

   fit_order
0          5
1          3
2          4
3          1
4          2  }
{
    "roc_auc": 0.9953107196047976,
    "accuracy": 0.9857581396380601,
    "balanced_accuracy": 0.9857581396380601,
    "mcc": 0.9715323475857583,
    "f1": 0.9857989771520281,
    "precision": 0.9829803514379707,
    "recall": 0.9886338137641597
}
1.3944078538112597e-05